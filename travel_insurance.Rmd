---
title: "Analyzing Travel Insurance"
author: "Kah Jun Lim"
date: "10/2/2019"
output:
  github_document: default
  html_document:
    code_folding: hide
    number_sections: no
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The purpose of this project is to perform exploratory data analysis(EDA) on travel insurance dataset and identify factors that might affect whether a claim is made.

# Loading the Data and Dependencies

```{r Data and Dependencies, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(tidyr)
library(forcats)
library(scales)
library(gridExtra)
library(caret)
library(rpart)
library(rpart.plot)
library(pROC)
library(randomForest)
library(xgboost)
set.seed(0)

travel_insurance_full <- read.csv("data/travel_insurance.csv")
head(travel_insurance_full)
```

```{r Rename columns}
colnames(travel_insurance_full) <- c("agency", "agency.type", "distribution.channel", "product.name", "claim", 
                                     "duration", "destination", "net.sales", "commision", "gender", "age")
```

# Exploratory Data Analysis

## Handling Missing/Bad Data

```{r Simple Statistics}
summary(travel_insurance_full)
```

There are 10 features in this data set, 4 of them are numeric and the others are factors. The only column with missing data is Gender, where a large portion of the values are missing. I notice that there are non-positive values for duration, which does not make sense. There are also durations that are excessively long. For age, the highest value is 118.

```{r Looking at bad data}
travel_insurance_full %>%
  filter(duration <= 0 | duration >= 1000) %>%
  nrow
travel_insurance_full %>%
  filter(age > 100) %>%
  nrow
```

Since there is only a relatively small proportion of the data contains unreasonable duration and uncommon age, I remove these data points for the purpose of this project. To reduce the levels of destination, I  also group the countries with less than 50 observations as Others.

```{r Preprocessing}
travel_insurance <- travel_insurance_full %>%
  filter(duration > 0 & duration < 1000 & age <= 100) %>%
  group_by(destination) %>%
  mutate(freq = n()) %>%
  ungroup() %>%
  mutate(destination = ifelse(freq > 50, as.character(destination), "OTHERS")) %>%
  transform(destination = as.factor(destination))
levels(travel_insurance$gender) <- c("Unknown", "F", "M")
```

We can now move on to look at distribution of some of the features.

## Looking Into Factor Variables

### Claim

```{r Claim Distribution}
ggplot(travel_insurance, aes(x = claim)) +
  geom_bar()
```

From the graph above, we see that only an extremely small number of policies have claims, which is what one would expect. In this case, claims occured can be viewed as anomalies or rare events. Moving forward, it is better to look at the proportion instead of the frequencies.

### Agency Type

```{r Agency Type}
ggplot(travel_insurance, aes(x = agency.type, fill = claim)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = percent_format())
```

We see that policies sold by airlines are more likely to observe claim compared to policies sold via travel agencies.

### Gender

```{r Gender}
ggplot(travel_insurance, aes(x = gender, fill = claim)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = percent_format())
```

It is interesting that Unknown gender experienced less claims than M or F. One possible explanation is that the higher the chance of claims, the more formal the policy would be, which includes recording more information of the policyholder.

```{r plot_top_n, include=FALSE}
# Helper functions to plot top n for factors with many levels
plot_top_n <- function(df, var, n = 10, title = "") {
  colnames(df)[colnames(df) == var] <- "var"
  freq_by_var <- df %>%
    group_by(var, claim = as.factor(claim)) %>%
    summarise(freq = n()) %>%
    spread(key = claim, value = freq, fill = 0) %>%
    mutate(total = Yes + No) %>%
    ungroup %>%
    top_n(n = n, wt = total) %>%
    select(-total) %>%
    gather(key = "claim", value = "freq", Yes, No)
  p <- ggplot(freq_by_var, aes(x = reorder(var,-freq), y = freq, fill = claim))
  p1 <- p +
    geom_bar(stat = "identity") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    xlab(var)
  p2 <- p +
    geom_bar(stat = "identity", position = "fill") +
    scale_y_continuous(labels = percent_format()) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    xlab(var)
  grid.arrange(p1, p2, ncol = 2, top = title)
}
```

### Destination

```{r Destination}
plot_top_n(travel_insurance, "destination", n = 10, title = "Top 10 Destinations")
```

In this dataset, the country with the most entry is Singapore, which also has the highest proportion of claims observed. Among the top 10 countries in this data, US, China, and Australia also have relatively higher proportion of claims compared to the others.

### Agency
```{r Agency}
plot_top_n(travel_insurance, "agency", title = "Top 10 Agencies")
```

The agencies with high proportion of claims observed are C2B and LWC.

### Product
```{r Product}
plot_top_n(travel_insurance, "product.name", title = "Top 10 Products")
```

Among the top 10 products in this data, Annual Silver Plan, Silver Plan, and Brownze Plan has the highest claims proportions.


## Numeric Variables
```{r Numeric variables}
p1 <- ggplot(travel_insurance, aes(x = duration, y = claim)) +
  geom_point(alpha = 0.05, shape = 1)
p2 <- ggplot(travel_insurance, aes(x = net.sales, y = claim)) +
  geom_point(alpha = 0.05, shape = 1)
p3 <- ggplot(travel_insurance, aes(x = commision, y = claim)) +
  geom_point(alpha = 0.05, shape = 1)
p4 <- ggplot(travel_insurance, aes(x = age, y = claim)) +
  geom_point(alpha = 0.05, shape = 1)
grid.arrange(p1, p2, p3, p4, ncol = 2)
```

There doesn't seem to be any obvious pattern in the four plots. Nevertheless, the interaction between these features with each other or the factor variables might become useful.

# Building Models

## Train-Test Splitting

Before we begin building models, we want to split the data into training set and testing set.

```{r show_prop, include=FALSE}
show_prop <- function(df) {
  df %>%
    group_by(claim) %>%
    summarise(n = n()) %>%
    mutate(prop = n / sum(n))
}
```

```{r}
train.index <- createDataPartition(travel_insurance$claim, p = 0.7, list = FALSE)
train <- travel_insurance[train.index,]
test <- travel_insurance[-train.index,]
show_prop(travel_insurance)
show_prop(train)
show_prop(test)
```

## Tree-Based Models

### Simple Decision Trees
```{r Simple Decision Tree 1}
tree.simple <- rpart(claim ~ ., train)
```

Since the data is very imbalanced, our simple model would always classify policy as no claim. The model achieved a high accuracy of 98.5%, implying accuracy is a bad metric for performance. Dealing with imbalanced data, we can do the following two things:

1. oversample the minority data to generate a balanced sample

2. use F1-score as our metric as it considers both precision and recall

```{r evaluate_tree, include=FALSE}
evaluate_tree <- function(tree, df, show.cm = TRUE, rf = FALSE) {
  if (rf) {
    pred <- predict(tree, newdata = df)
  }
  else {
    pred <- factor(ifelse(predict(tree, newdata = df)[,"Yes"] > 0.5, "Yes", "No"), levels = c("No", "Yes"))
  }
  cm <- confusionMatrix(pred, df$claim)
  if (show.cm) print(cm$table)
  ppv <- precision(pred, df$claim, relevant = "Yes")
  tpr <- recall(pred, df$claim, relevant = "Yes")
  c(Accuracy = (cm$table[1,1] + cm$table[2,2]) / sum(cm$table),
    Precision = ppv, 
    TPR = tpr, 
    F1 = 2 * ppv * tpr / (ppv + tpr))
}
```

```{r performance for simple tree}
evaluate_tree(tree.simple, test)
```

Since all the prediction are negative, none of the true positive are correctly predicted. As a results, the true positive rate is 0.

### Simple Decision Tree on Resampled Balanced Data

```{r draw_balanced_sample, include=FALSE}
draw_balanced_sample <- function(train, n) {
  train.yes <- sample_n(train %>% filter(claim == "Yes"), n, replace = TRUE)
  train.no <- sample_n(train %>% filter(claim == "No"), n, replace = TRUE)
  rbind(train.yes, train.no)
}
```

```{r Simple Decision Tree 2}
train.balanced <- draw_balanced_sample(train, 10000)
show_prop(train.balanced)
tree.simple.balanced <- rpart(claim ~ ., train.balanced)
evaluate_tree(tree.simple.balanced, test)
```

Although the accuracy drops, we see that the model is now predicting some of the true positive correctly. The problem now is that the model is also predicting a lot of the true negative as positive. Overall, the F1-score is pretty low. This is due to overfitting, and to tackle this problem, we can try using some ensemble methods like bagging and boosting.

### Random Forest Model

```{r Random Forest}
set.seed(0)
rf.balanced <- randomForest(claim ~ ., train, ntree = 300,
                            sampsize = c(500, 200),
                            strata = train$claim)
rf.balanced
evaluate_tree(rf.balanced, test, rf = TRUE)
```

```{r}

```

